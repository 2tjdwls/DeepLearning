Week4

<Deep Neural Network>

Logistic Regression : Shallow model
1 hidden layer(2 layer Neural Network) : Shallow model

3 hidden layer, 1 output 1ayer NN
L = #layers = 4
n^[l] = #units in layer l
n^[1] = 5, n^[2] = 5, n^[3] = 3, n^[4] = n^[L] = 1, n^[0] = n_x = 3
a^[l] = activations in layer l = g^[l](z^[l])
w^[l] = weights for z[l]
b^[l] = bias for z[l]
x = a^[0], y^ = a[L]

- Forward propagation

Input a^[l-1]
Output a^[l]
cache(z^[l]) = w^[l], b^[l]

z^[l] = w^[l]a^[l-1] + b^[l]
a^[l] = g^[l](z^[l])

→ Vectorizaiton
z^[l] = w^[l]*A^[l-1] + b^[l]
A^[l] = g^[l](z^[l])

X = A^[0] → ㅁ → ㅁ → ㅁ → : Initialization
a^[0] : train sample의 input feature
A^[0] : train set의 input feature

- Back propagation

Input da^[l]
Output da^[l-1], dW^[l], db^[l]

dz^[l] = da^[l]*g^[l]'(z^[l]) -> * : element-wise product
dw^[l] = dz^[l]a^[l-1]
db^[l] = dz^[l]
da^[l-1] = w^[l][T]dz[l]
-> dz^[l] = w^[l+1][T]dz^[l+1]*gz^[l]'(z^[l])

-> Vectorization
dz^[l] = dA^[l]*g^[l]'(z^[l])
dw^[l] = (1/m)dz^[l]A^[l-1].T
db^[l] = (1/m)np.sum(dz^[l],axis=1,keepdims=True)
dA^[l-1] = w^[l][T].dz^[l]

Back propagation Initialization : da^[L]로 초기화

- Debugging tool
-> 종이에 직접 Dimension을 써가면서 확인하는 방법

L=5인 NN
n^[0]=n_x=2, n^[1]=3, n^[2]=5, n^[3]=4, n^[4]=2, n^[5]=1
z^[1] = w^[1]x + b^[1]
z^[1] : (3,1) = (n^[1],1)
x : (2,1) = (n^[0],1) = (n_x,1)
w^[1] : (3,2) = (n^[1], n^[0])
-> w^[l] : (n^[l], n^[l-1]) -> dw^[l] : 차원 같음
-> b^[l] : (n^[l], 1) -> db^[l] : 차원 같음

z^[2] = w^[2]a^[1] + b^[2]
a^[1] : (3,1) = Dimension of z^[1]
z^[2] : (5,1)
w^[2] : (5,3) = (n^[2], n^[1])

a^[l] = g^[l](z^[l]) z^[l]과 a^[l]은 차원 같음

→ Vectorization (set)
    [    :          :        :       :     ]
Z = [ z^[1](1)   z^[1](2)   ...   z^[1](m) ]
    [    :          :        :       :     ]

Z^[1] = A^[1] : (n^[1],m)
W^[1] : (n^[1], n^[0])
X : (n^[0], m) -> train set
b^[1] : (n^[1],1) -> Broadcasting -> (n^[1], m)

-> Z^[l] = A^[l] : (n^[l],m)
-> W^[l] : (n^[l], n^[l-1])
-> b^[l] : (n^[l],1)
-> X : (n^[0], m)
-> dZ^[l] : (n^[l], m)
-> dA^[l] : (n^[l], m)

layer l : w^[l], b^[l]

-> Forward : Input a^[l-1] / Output a^[l]
z^[l] = w^[l]a^[l-1] + b^[l] (cache : z^[l])
a^[l] = g^[l](z^[l])

-> Backward : Input da^[l], z^[l] / Output da^[l-1], dw^[l], db^[l]

* Forward propagation
Input feature a^[0] -> (w^[1], b^[1]) -> a^[1] -> (w^[2], b^[2]) -> a^[2] -> ... -> a^[l] = y^ = a^[l] : Output
                             ↓                          ↓                             ↓            ↓
                       cache : z^[1]              cache : z^[2]                 cache : z^[l]      ↓
* Back propagation                                                                    ↓            ↓
         (w^[1],b^[1],dz^[1]) <- da^[1] <- (w^[2],b^[2],dz^[2]) <- ... <- (w^[l],b^[l],dz^[l]) ← da^[l] : Input gradient
                    ↓                                 ↓                              ↓
                  dw^[1]                            dw^[2]                         dw^[l] -> update : w^[l] := w^[l] - αdw^[l]
                  db^[1]                            db^[2]                         db^[l] -> update : b^[l] := b^[l] - αdb^[l]

- Hyperparameter

Parameters : W^[1], b^[1], W^[2], b^[2], W^[3], b^[3] ...
Hyperparameter : learning rate α, #interations, #hidden layers : L, #hidden units n^[1], n^[2]..., Activation function -> w,b를 통제하는 변수

Later lecture's hyperparameter : Momentum, Minibatch size, Regularizations...
-> 딥러닝은 Hyperparameter가 매우 많다

Hyperparameter를 미리 예측하는 것은 불가능에 가깝다 -> 경험적으로 많은 시도를 통해 적합한 hyperparameter를 결정할 수 있음