Week3

<Neural Networks Overview>

Logistic Regression에서 x, w, b를 이용해서 z를 구하고 z를 이용해서 a = σ(z)를 구하고 이를 통해 Loss function을 구함 -> 여기서 node는 z를 구하는 node와 a를 구하는 node 2개로 구성
신경망은 여러개의 sigmoid function을 쌓아서 만들 수 있음
각 층(layer)을 위첨자 [i]로 구분해서 사용 -> layer[1] : z^[1] = w^[1]*x + b^[1], layer[2] : z^[2] = w^[2]*x + b^[2]

* 신경망에서는 z와 a를 여러번 계산하고 마지막으로 최종 y^과 Loss function을 계산함

x1, x2, x3 ... 이 쌓여 있는 건 Input layer(입력 층)이라고 함
hidden layer(은닉 층) : train set은 입력 값 x와 출력 값 y로 구성되어 있음, 은닉 층은 train set에 없음(train set에서 무슨 값인지 모름) -> 은닉 층 : train set에서 볼 수 없는 층
마지막으로 하나의 node를 Output layer(출력 층)이라고 함 : 예측 값인 y^을 계산함

Input layer : a^[0] = x -> a^[0]_1 = x_1, a^[0]_2 = x_2, a^[0]_3 = x_3, a^[0]_4 = x_4
2nd layer : a^[1]_1, a^[1]_2, a^[1]_3, a^[1]_4
Output layer : a^[2]
-> 2 layer NN (관례적으로 Input layer는 count하지 않음, hidden layer가 layer1, output layer가 layer2임)

a^[l]_i -> [l] : layer, i : node in layer
a^(i) -> example i (sample i)

-> m개의 Input_sample에 대한 계산 -> Vectorizing 이용 (각 sample은 n_x개의 feature를 가지고 있음)

 * Vectorizing
     [  :     :         :  ]
 X = [x^(1) x^(2) ... x^(m)] -> (n_x X m) matrix : node수 (sample의 feature수) X 훈련 샘플 수, (X : train set, x : train sample)
     [  :     :         :  ]

        [   :        :             :   ] -> (1,1) = 첫 hidden unit의 첫 훈련 샘플의 z값
Z^[1] = [z^[1](1) z^[1](2) ... z^[1](m)] -> (number of node X m) matrix : node수 X 훈련 샘플 수
        [   :        :             :   ]

        [   :        :             :   ] -> (1,1) = 첫 hidden unit의 첫 훈련 샘플의 activation값
A^[1] = [a^[1](2) a^[1](2) ... a^[1](m)] -> (number of node X m) matrix : node수 X 훈련 샘플 수
        [   :        :             :   ]

Input을 Column으로 stacking하면 Output도 Column으로 stacking되서 나온다

* Activation function
1. sigmoid function : a = σ(z) = 1/(1+e^-z) : 이진 분류 문제의 출력층에서 (y∈{0,1}일 때 0≤y^≤1으로 할 때) 사용
2. hyperbolic tangent function : a = tanh(z) = (e^z-e^-z)/(e^z+e^-z) : 값이 -1 ~ +1 사이에 있기 때문에 평균값이 0에 더 가까워진다, 학습 알고리즘을 훈련할 때 평균값의 중심을 0으로 할 때도 좋음
                                                                       데이터의 중심을 0으로 할 수 있음, 다음 층의 학습을 더 쉽게 해줌 
                                                                       * 시그모이드와 tanh는 z가 매우 작거나 크면 기울기가 0에 수렴해서 gradient descent가 느려질 수 있음
3. ReLU function : a = max(0,z) : z가 음수이면 도함수=0, 양수이면 도함수=1
                                  * 은닉층에 어떤 activation function을 써야할 지 모르겠으면 ReLU를 써라 -> 가장 많이 사용하고 잘 작동함
4. Leaky ReLU function : a = max(0.01z,z) : z가 음수이면 약간의 도함수를 가지고, 양수이면 도함수=1 -> z가 음수일 때 도함수는 학습 알고리즘의 변수로 지정 가능(0.01은 예시)

* 비선형 활성화 함수가 필요한 이유?
ex)
Given x :
z^[1] = W^[1]*x + b^[1]
a^[1] = z^[1] -> Linear activation Function or Identity activation Function
z^[2] = W^[2]*x + b^[2]
a^[2] = z^[2]

-> a^[1] = z^[1] = W^[1]*x + b^[1]
-> a^[2] = z^[2] = W^[2]*a^[1] + b^[2]
-> a^[2] = W^[2]*(W^[1]*x + b^[1]) + b^[2]
-> a^[2] = W^[2]*W^[1]*x + W^[2]*b^[1] + b^[2]  (W^[2]*W^[1] = W', W^[2]*b^[1] + b^[2] = b')
-> a^[2] = W'*x + b' 

* 선형 활성화 함수를 사용하면 신경망은 입력의 선형식만을 출력한다 -> 층이 얼마나 많든간에 출력은 은닉층이 얻는 것과 같다
* 선형 은닉층은 쓸모가 없다 -> 단, 회귀문제에 대한 머신러닝 기법에서는 사용 (출력값인 y^이 실수일 때 ex)집 값 예측 모델)

- Activation function의 도함수
1. sigmoid function : g(z) = σ(z), g'(z) = g(z)*(1-g(z))
2. hyperbolic tangent function : g(z) = tanh(z), g'(z) = 1-(tanh(z))^2
3. ReLU and Leaky ReLU : g(z) = max(0,z), g'(z) = 0 (if z<0), 1 (if z>0), undefined or 0 or 1 (if z=0)
                         g(z) = max(0.01z,z), g'(z) = 0.01 (if z<0), 1 (if z>0), undefined or 0.01 or 1 (if z=0)

- Parameter
w^[1] : (n^[1], n^[0])
b^[1] : (n^[1], 1)
w^[2] : (n^[2], n^[1])
b^[2] : (n^[2], 1)

- Cost Function : J(w^[1], b^[1], w^[2], b^[2]) = 1/m*Σ(i=1~m)L(y^,y)

- Gradient descent
Repeat {
    Compute predict (y^(i), i=1~m)
    dw^[1] = dJ/dw^[1], db^[1] = dJ/db^[1]...
    w^[1] = w^[1] - αdw^[1]
    b^[1] = b^[1] - αdb^[1]
    ...
}

- Forward propagation
z^[1] = w^[1]*X + b^[1]
A^[1] = g^[1](z^[1])
z^[2] = w^[2]*A^[1] + b^[2]
A^[2] = g^[2](z^[2]) = σ(z^[2])

- Back propagation
dz^[2] = A^[2] - y
dw^[2] = (1/m)*dz^[2]*A^[1].T
db^[2] = (1/m)*np.sum(dz^[2], axis=1, keepdims=True) # axis=1 : row끼리 연산, keepdims=True : Rank 1 array 방지
dz^[1] = w^[2].T*dz^[2] * g^[1]'*(z^[1]) # element-wise product -> w^[2].T*dz^[2] = (n^[1], m) ,g^[1]'(z^[1]) = (n^[1], m)
dw^[1] = (1/m)*dz^[1]*X.T = (1/m)*dz^[1]*A^[0].T
db^[1] = (1/m)*np.sum(dz^[1], axis=1, keepdims=True)

- Dimension
w^[2] = (n^[2], n^[1])
z^[2], dz^[2] = (n^[2], 1)
z^[1], dz^[1] = (n^[1], 1)
-> dz^[1] = w^[2].T*dz^[2] * g^[1]'*(z^[1]) -> (n^[1], n^[2])*(n^[2], 1) * (n^[1], 1) = (n^[1], 1)*(n^[1], 1) = (n^[1], 1)

- Initialization
신경망에서 w^[1], w^[2]를 zero Initialization하는 것은 symmetric을 발생시킴
-> a^[1]_1 = a^[1]_2, dz^[1]_1 = dz^[1]_2
* 수학적 귀납법을 이용하면 각 training의 반복마다 두 은닉 유닛이 항상 같은 함수를 계산함
* epoch를 몇 번 하든지, 은닉층이 몇개이든지 상관 없이 zero Initialization은 symmetric problem을 유발시키므로 실제로는 은닉층이 하나 있는 것과 같음

- Random Initialization
w^[1] = np.random.randn((2,2)) * 0.01 # Gaussian random variable을 2x2 matrix로 생성하고 0.01곱해서 매우 작은 값으로 Random하게 Initialization
b^[1] = np.zero((2,1)) # b는 zero Initialization해도 symmetric을 발생시키지 않음 -> symmetric braking problem
w^[2] = ...
b^[2] = ...

* 가중치의 초기값에 0.01을 곱하는 이유
가중치의 초기값은 매우 작은 값으로 하는 것이 좋다 -> w가 커지면 z가 매우 커지거나 매우 작아지므로 activation function에서 gradient의 문제가 생길 수 있음
                                                 ex) sigmoid or tanh에서 gradient = 0 -> gradient descent 속도 ↓ -> learning spped ↓
                                                 

