Week.2

<Logistic Regression as a Neural Network>

#1
신경망 구현
-m개의 학습 표본을 가진 학습 set가 있으면 m개의 학습 표본에 대해 for문을 돌리면서 전체 학습 set에 적용시키지 않는다
-신경망으로 계산할 때 순방향경로(순전파), 역방향경로(역전파)를 이용
*로지스틱 회귀 : 이진 분류를 위한 알고리즘, 지도학습 문제에서 결과값 레이블 Y의 값이 0or1인 경우 사용되는 학습 알고리즘
ex) 고양이 이미지를 input으로 해서 1(cat) vs 0(non cat) 로 lable하고자 함
이미지의 픽셀 채도값을 특징벡터(Feature vector)로 변환 -> 전체 픽셀값을 하나의 입력 특징벡터 x에 펼침 -> 특징벡터 정의 : 각 픽셀값을 특징벡터의 요소(feature)로 나열 -> 이미지가 64x64라면, 특징벡터는 이미지의 RGB 모두를 포함하기 때문에 64x64x3개의 요소를 가짐 -> n_x=12288
* 하나의 학습표본은 순서쌍 (x,y)로 표기 : x는 x차원을 가진 특징벡터, y는 0/1의 값을 가지는 레이블, 학습 set는 m개의 학습표본(sample)을 가짐 (m_train)
* Test set에서는 test표본의 수를 m_test로 표현
* 대문자 X : 행렬로 정의 [행 : 학습표본이 몇개의 요소(feature)로 되어있는가(feature수 : n_x), 열 : 학습표본(sample)이 몇개인가(sample수 : m)] -> n_x*m행렬 (X.shape = (n_x,m))
* 출력행렬 Y : 1행 m열 -> 1*m행렬 (Y.shape = (1,m))

#2
-로지스틱 회귀
y^ : Y의 평균(Y는 주어진 입력 특성 X가 되는 확률로 만든다)
ex) X가 이미지일 때 y^은 '이 이미지가 고양이일 확률은?'
로지스틱 회귀분석법의 매개변수 W는 x차원 벡터, 실수 b도 x차원 벡터 -> y^ = w*x+b로 둔다면, 이는 선형회귀에서는 사용되지만 이항분류문제에서는 사용하지 않음 (y^이 Y가 1일 확률이 되도록 만드는 것이 좋기 때문)
따라서 0<=y^<=1 이어야 함, y^=σ(w*x+b) : Sigmoid function
*Sigmoid function : σ(z)는 z가 +∞로 가면 1에 수렴, -∞로 가면 0에 수렴, 원점에선 0.5의 값을 가지는 함수

#3
로지스틱 회귀 모델의 매개변수 w와 b를 주어진 m개의 훈련 샘플로 학습할 때 이 훈련 세트를 바탕으로 출력한 y^(i)의 예측값이 훈련 세트에 포함된 참값 y(i)에 가까워지도록 해야함
위 첨자 (i) : i번째 훈련 샘플에 관한 데이터임을 뜻함
- 손실함수(Loss function or Error function)
L(y^,y) : 출력된 y의 예측값(y^)과 참값 y사이에 오차가 얼마나 큰지 측정
만약 Error function을 1/2(y^-y)^2으로 정의한다면, 이는 매개 변수들을 학습하기 위해 풀어야할 최적화 함수가 볼록하지 않고, 여러개의 지역 최솟값을 갖기 떄문에 경사 하강법에서 전역 최솟값을 찾지 못할 수 있기 때문에 로지스틱 회귀에서는 위 Error function정의를 사용하지 않는다 -> 따라서 위와 비슷하지만, 최적화 함수가 볼록해지는 Error function 정의
L(y^,y) = -(ylog(y^) + (1-y)log(1-y^))
* Error function은 훈련 샘플 하나에 관하여 정의돼서 그 하나가 얼마나 잘 예측되었는지 측정
- 비용함수(Cost function
* 훈련 세트 전체에 대해 얼마나 잘 추측되었는지 측정해주는 함수
J(w,b)=1/m*Σ(i=1~m)L(y^(i),y(i)) : 매개변수 w와 b에 대해 Error function을 각각의 훈련 샘플에 적용한 값의 합들의 평균
y^은 로지스틱 회귀 알고리즘이 정해진 매개 변수들 w와 b를 적용해서 출력하는 값, 위의 Error function을 식에 대입하면 -> -1/m*Σ(i=1~m)(y(i)logy^(i)+(1-y(i)log(1-y^(i))))
* Error function은 하나의 훈련샘플에 적용을 한다, Cost function은 매개 변수의 Cost처럼 작용 -> 로지스틱 회귀 모델을 학습한다 = Cost function J를 최소화해주는 w와 b를 찾는 것
 
 #4
 Cost function은 Training set에서 파라미터 w와 b가 얼마나 잘 하는지를 측정 -> w,b가 J(w,b)를 최소화시키는 값을 찾는다
 w와 b를 매개변수로 하는 볼록한 평면 모양의 J(w,b)를 보여줌 (단, w는 더 높은 차원일 수 있지만, 표현을 위해 실수라고 가정) -> 여기서 J(w,b)는 볼록함수이고, 이 때 J(w,b)가 최솟값을 가지는 지점을 찾아야함 (J(w,b)평면과 wb평면간 높이는 각 지점에서의 J(w,b)의 값을 나타냄)
 파라미터 w,b의 적합한 값을 찾기 위해선 w와 b를 최초값으로 초기화(Initialization)해야함, 로지스틱 회귀분석에서는 거의 모든 초기화 방법이 잘 구현됨, 일반적으로 0으로 초기화, 위 함수는 볼록함수이기 때문에 어느 지점에서 초기화를 하든 똑같거나 비슷한 점에 도달하게 될 것
 Gradient descent(기울기 강하)는 처음 시작점에서 시작해서 최대한 빠른 속도로 가장 기울기가 높은 내리막길 방향으로 이동, 기울기 강하는 결과적으로 이상적으로는 전역최적점 또는 그 근접한 지점에 도달 
 직관적으로 이해하기 위해 w만을 매개변수로 하는 1차원 Cost function도입 -> J(w)는 단지 좌표평면에서 볼록함수의 모양
 기울기 강하가 진행될수록 w를 update시킨다
 Repeat {
 	w := w-αdJ(w)/dw // :는 update, α는 learning rate(한번에 얼만큼 기울기 강하를 진행할 수 있는지 조절하는 parameter
 }					 // derivative : update 또는 w에 얼마나 변화를 줄지 여부를 알려주는 값)
Gradient descent를 하는 programming을 할 땐 dw 변수로 derivative항을 표시한다
실제로 Cost function은 w와 b를 매개변수로 하는 함수이기 때문에 실제 도입하는 업데이트는 -> Gradient descent : w := w-αdJ(w,b)/dw / b := b-αdJ(w,b)/dw
만약 J가 w,b뿐만 아니라 그 이상의 매개변수를 가진다면 편미분 표기를 사용(∂) -> Programming에서는 ∂J(w,b)/∂w = dw / ∂J(w,b)/∂b = db 로 표시

#5
***Back propagation을 programming할 때 우리가 최적화 시키고 싶은 결과값 변수(Computation graph에서 마지막 node)의 derivative를 dvar라고 표시할 것
*** dv = dJ/dv, da = dJ/da로 표시

#6
로지스틱 회귀분석을 위한 Gradient descent를 Computation graph를 이용해서 계산하는 fomula

<Python and Vectorizaiton>

#1
Vectorization : Code에서 for loop를 제거하는 기술
딥러닝 알고리즘은 실행부분에서 큰 데이터세트에서 트레이닝 하는 것에 특화되어 있음 -> 코딩을 빠르게 진행하는 것이 매우 중요
* for loop를 제거하면서 결과 값이 나오는 시간을 줄일 수 있음
ex) 같은 code를 Non-Vectorized, Vectorized version으로 비교
z = w^T*x + b, w,x : column matrix, w∈R^n_x, x∈R^n_x
1. Non-Vectorized : 속도가 매우 느림
z == 0
for i in range(n_x) :
	z += w[i]*x[i]
z += b
2. Vectorized : 속도가 매우 빠름
z = np.dot(w,x)+b # w*x + b
ex) 러닝 시간 측정
import numpy as np
# Vectorized version
a = np.array([1,2,3,4])
print(a)
import time
a = np.random.rand(1000000)
b = np.random.rand(1000000)
tic = time.time()
c = np.dot(a,b) # c = a*b
toc = time.time()
print(c)
print("Vectrized version" + str(1000*(tic-tic))+"ms")
# Non-vectorized version
c = 0
tic = time.time()
for i in range(1000000) : 
	c += a[i]*b[i]
toc = time.time()
print(c)
print("for loop :" + str(1000*(toc-tic))+"ms")
- GPU vs CPU
* GPU와 CPU 모두 Parallelization instruction이 있음 -> SIMD instruction이라 함(Single instruction multiple data)
built-in function(np.function이나 for loop가 필요 없는 기능)을 이용하면 python이 parallelism을 활용할 수 있게 계산을 빨리 처리함
* GPU는 특별히 SIMD caculate에 뛰어남, CPU가 나쁘다는건 아님

#2
* 가능한 한 일정한 for loop를 피하라, 단 항상 for loop를 피할 수 있는 것은 아님
* 내장함수로 처리가 가능하거나 계산을 할 때 다른 방법을 찾으면 속도를 높힐 수 있음
ex.1)
u = A*v
u_i = Σ(i~)A_ij*v_j

-> Non-vectorized version
u = np.zeros((n,1))
for i in range(len(i)) :
	for j in range(len(j)) :
		u[i] = A[i][j]*v[j]

-> Vectorized version
u = np.dot(A,v) # for loop 제거

ex.2)
v = [ v_1 ] -> u = [ e^v_1 ]
	[ v_2 ]		   [ e^v_2 ]
	[ v_3 ]		   [ e^v_3 ]
	[ ：  ]		   [   ：  ]
	[ v_n ]		   [ e^v_n ]

-> Non-vectorized version
u = np.zeros((n,1))
for i in range(n) :
	u[i] = math.exp(v[i])

-> Vectorized version
import numpy as np

u = np.exp(v)

** numpy built-in function
1) np.log(v) : v의 모든 요소에 log
2) np.abs(v) : v의 모든 요소에 절댓값
3) np.maximum(v,0) : v의 모슨 요소의 max값을 0으로
4) v**2 : v의 모든 요소를 제곱
5) 1/v : v의 모든 요소를 역수

#3
로지스틱 회귀에 vectorization 도입

z(1) = w^T*x(1) + b		z(2) = w^T*x(2) + b 	z(3) = w^T*x(3)
a(1) = σ(z(1))			a(2) = σ(z(2)) 			a(3) = σ(z(3))

	[  :     :     :   ]
X = [ x(1) x(2) … x(m) ] -> (n_x,m) matrix
	[  :     :     :   ]

Z = [ z(1) z(2) … z(m) ] = w^T*X + [b b … b] = w^T*x(1) + b, w^T*x(2) + b, … ,w^T*x(3) + b
											  (= w^T*[ x(1) x(2) x(3) ] )
z = np.dot(w^T,X) + b # 실수 b를 vector에 더할 때는 python이 자동으로 실수 b를 1xm vector로 확장시킴(1xm row vector) -> Broadcasting

#4
dz(1) = a(1)-y(1) 		dz(2) = a(2)-y(2) 		... 	dz(m) = a(m)-y(m)
dZ = [dz(1) dz(2) ... dz(m)] -> 1xm matrix(=m dimensional vector)
A = [a(1) a(2) ... a(m)] 	Y = [y(1) y(2) ... y(m)] -> dZ = A-Y
db = 1/m*Σ(i=1~m)dz(i)
dw = 1/m*X*dz^T

-> Non-vectorized version
dw = 0					db = 0
dw += x(1)*dz(1)		db += dz(1)
dw += x(2)*dz(2)		db += dz(2)
	   :					:
dw /= m					db /= m

-> Vectorized version
db = 1/m*np.sum(dz)
dw = 1/m*X*dz^T

* Logistic Regression Vectorized version code
np.dot(w^T,x) + b # Z = w^T*X + b
A = σ(Z)
dz = A-Y
dw = 1/m*X*dz^T
db = 1/m*np.sum(dz)

w := w-αdw
b := b-αdb

#5
Broadcasting

		  Apples	Beef	Eggs	Potatoes
Carb  	[ 56.0		0.0		4.4		68.0	]
Protein [ 1.2		104.0	52.0	8.0		]	-> 3x4 data matrix
Fat		[ 1.8		135.0	99.0	0.9		]
-> 위 3x4 data matrix에서 각 음식별 총 칼로리와, 탄수화물, 단백질, 지방이 총 칼로리에서 차지하는 퍼센트를 for loop없이 계산 (row data만 꺼내와서 계산) -> Vectorizaiton 이용
-> 위 3x4 data matrix를 matrix A로 정의 후 row끼리 더해서 총 칼로리를 구하고, 각 row elements를 총 칼로리로 나누는 계산 -> python code 2줄로 구현

code)
import numpy as np

A = np.array([[56.0, 0.0, 4.4, 68.0],
			  [1.2, 104.0, 52.0, 8.0],
			  [1.8, 135.0, 99.0, 0.9]])

cal = A.sum(axis=0) 	# row로 더하는 방법, axis=1이면 column으로 더하는 방법임 **

percentage = 100 * A/cal.reshape(1,4) # numpy array여서 이미 1x4 column matrix인데 reshape해주는 이유? -> spyder로 검토해보니까 type같고 결과값도 cal로하나 reshape으로 하나 똑같음, cal은 []이고 reshape한건 [[]]임 -> 차이점 ?
									  # 해결! : 같은 1x4 matrix이지만 reshape을 굳이 해준 이유는 matrix의 dimensional을 제대로 파악하지 못할 때 정확한 계산을 할 수 있게 실수를 줄여주는 방어막 역할 + reshape은 order one operation이어서 매우 cheap한 function, 부담이 없음
print(percentage) 					  # A는 3x4 matrix인데 1x4 matrix로 나눔 -> How?

How)
ex.1 : 4x1 matrix에 실수를 더하면 python은 자동으로 실수는 4x1 matrix form으로 확장시켜서 계산함
	   [1]		   [1]	 [100]	 [101]
	   [2] + 100 = [2] + [100] = [102]
	   [3]		   [3]	 [100]	 [103]
	   [4]		   [4]	 [100]	 [104]
ex.2 : mxn matrix에 1xn matrix를 더하면 python은 자동으로 1xn matrix를 m번 반복시켜서 계산 수행
	   [1 2 3] + [100 200 300] 	=  [1 2 3] + [100 200 300] = [101  202  303]
	   [4 5 6] 					   [4 5 6]	 [100 200 300]   [104  205  306]
ex.3 : mxn matrix에 mx1 matrix를 더하면 python은 자동으로 mx1 matrix를 n번 반복시켜서 계산 수행
	   [1 2 3] + [100] = [1 2 3] + [100 100 100] = [101 102 103]
	   [4 5 6]   [200]   [4 5 6]   [200 200 200]   [204 205 206]
-> General Principle of Broadcasting

#6
Rank 1 array vs Vector
Broadcasting
강점 : 언어의 표현성 생성, 언어의 유동성(단순히 한 줄의 코드로도 많은 기능 구현 가능)
단점 : 유동성이 큰 만큼 Broadcasting의 구조와 특성에 대해 숙지하지 않으면 감지하지 힘든 버그나 이상하게 생긴 버그 유발 가능

ex) Python numpy의 덜 직관적인 효과 예시
import numpy as np

a = np.random.randn(5) # randint : 균일 분포의 정수 난수 1개 생성, rand : 0~1사이에서 난수 matrix array생성, randn : Gaussian standard normal distribution에서 난수 matrix array생성
print(a.shape) # (5,) : a는 Rank 1 array (데이터 구조) -> row vector도 아니고 column vector도 아님
print(a.T) # a의 Transpose는 a와 같은 값을 가짐 (전치가 안됨) -> Rank 1 array이기 때문
print(np.dot(a,a.T)) # Broadcasting에 의해 바깥쪽 product들이 matrix를 확장하지 않고 결과값이 단 하나의 값으로 나옴
-> 신경망을 구현하는 경우에는 데이터구조((n,) / Rank 1 array 등)를 사용하는 것이 아니라, vector로 지정해줌

a = np.random.randn(5,1) # Rank 1 array가 아닌 5,1을 지정해줌으로써 직관적으로 생성함
print(a) # 5x1 column vector로 출력
print(a.T) # a의 Transpose는 1x5 row vector로 출력 잘 됨
-> 이 때 vector형태는 [[]]꼴이고 Rank 1 array는 []꼴임 (위에 칼로리 예시코드에서 .sum()은 Rank 1 array출력, reshape은 vector로 출력했었다는 걸 깨달음***)
print(np.dot(a,a.T)) # 둘 다 vector니깐 Broadcasting에 의해 바깥쪽 product가 확장되어 연산

* assert(a.shape == (5,1)) # Vector의 dimension이 정확히 무엇인지 모를 때 Vector의 form을 정함 -> assert는 실행을 목적으로 하고 코드를 기록함

* 위에서 깨달았듯이 Rank 1 array를 의도치않게 만들었을 때 reshape을 이용해서 원하는 form의 vector로 바꿔줄 수 있음

* operater '*'는 element-wise multiplication(원소별 곱셈)을 한다. numpy.dot()이랑은 다르다. numpy.dot()은 broadcasting을 할 수 있음

#7
Logistic Regression
z = w^T*x + b
y = σ(z)
y^ = P(y=1|x)

if y = 1 : P(y|x) = y^
if y = 0 : P(y|x) = 1-y^
->  P(y|x) = (y^)^y*(1-y^)^(1-y) # P(y|x)의 올바른 정의
	(if y = 1 : P(y|x) = y^)
	(if y = 0 : P(y|x) = 1-y^)

위의 P(y|x)은 단순 증가 함수이기 때문에 maximizing을 통해 P(y|x)를 optimization함
-> log(P(y|x)) = y*log(y^)+(1-y)*log(1-y^) = -L(y^,y) : loss function의 (-)부호 붙힌 값
-> (-)부호 있는 이유 : 러닝 알고리즘을 트레이닝 시킬 때는 확률을 높이고 싶지만, Logistic Regression에서는 loss function을 minimizing해야 하기 때문 = log(P(y|x))를 maximizing해야 하기 때문

- Cost on m examples
P(lables in trainning set) = Π(i=1~m)P(y(i)|x(i)) # 따로 그린 트레이닝 세트는 IID로 그림(identically independently distribution : 독립 동일 분포)
-> log(P(~)) = Σ(i=1~m)log(P(y(i)|x(i))) = -L(y^(i),y(i))
-> 최대확률측정법(principle of maximum likelihood estimation)이라는 통계 원리 : 요약하자면 이 값을 최대화 시킬 수 있는 매개변수를 고르는 것 = 이 값을 최대화 시키는 것
-> = -Σ(i=1~m)L(y^(i),y(i))
-> Cost function(minimize) : J(w,b) = 1/m*Σ(i=1~m)L(y^(i),y(i)